{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original\n",
    "* https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 371661, 66 unique\n"
     ]
    }
   ],
   "source": [
    "with open('book.txt') as f:\n",
    "    data = f.read()\n",
    "    data = data.lower()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size}, {vocab_size} unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capítulo primeiro\n",
      "do título\n",
      "uma noite destas, vindo da cidade para o engenho novo, encontrei no trem da\n",
      "central um rapaz aqui do bairro, que eu conheço de vista e de chapéu.\n",
      "cumprimentou-me, sentou-se ao pé de mim, falou da lua e dos ministros, e\n",
      "acabou recitando-me versos. a viagem era curta, e os versos pode ser que não\n",
      "fossem inteiramente maus. sucedeu, porém, que, como eu estava cansado, fechei\n",
      "os olhos três ou quatro vezes; tanto bastou para que ele interrompesse a leitura e\n",
      "metesse os vers\n"
     ]
    }
   ],
   "source": [
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T(object):\n",
    "    \n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.data = data\n",
    "        # pointer\n",
    "        self.p = 0\n",
    "    \n",
    "    def get(self):\n",
    "        if self.p + self.seq_len + 1 > len(self.data):\n",
    "            self.p = 0\n",
    "        \n",
    "        X = [char_to_ix[char] for char in self.data[self.p  :self.p+self.seq_len  ]]\n",
    "        y = [char_to_ix[char] for char in self.data[self.p+1:self.p+self.seq_len+1]]\n",
    "        \n",
    "        self.p += self.seq_len\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    def __init__(self, vocab_dim, h_dim, seq_len, learning_rate, seed=None):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "        # vocabulary dimension\n",
    "        self.vocab_dim = vocab_dim\n",
    "        # hidden nodes dimension\n",
    "        self.h_dim = h_dim\n",
    "        # sequence length\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        #\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self._Wx = np.random.randn(self.vocab_dim, self.h_dim) * 1e-2\n",
    "        \n",
    "        self._Wh = np.random.randn(self.h_dim, self.h_dim) * 1e-2\n",
    "        self._Wy = np.random.randn(self.h_dim, self.vocab_dim) * 1e-2\n",
    "        \n",
    "        self._bh = np.zeros((1, self.h_dim))\n",
    "        self._by = np.zeros((1, self.vocab_dim))\n",
    "        \n",
    "        self._mWx, self._mWh, self._mWy = np.zeros_like(self._Wx), np.zeros_like(self._Wh), np.zeros_like(self._Wy)\n",
    "        self._mbh,  self._mby = np.zeros_like(self._bh), np.zeros_like(self._by)\n",
    "        \n",
    "    def _forward(self, X, y, hprev):\n",
    "        \"\"\"\n",
    "        X (seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        h = np.dot(X, self._Wx) + np.dot(hprev, self._Wh) + self._bh\n",
    "        h = np.tanh(h)\n",
    "        \n",
    "        y_pred = np.dot(h, self._Wy) + self._by\n",
    "        y_pred = self._softmax(y_pred)\n",
    "        \n",
    "        loss = -np.log(y_pred[0,y])\n",
    "        \n",
    "        return h, y_pred, loss\n",
    "    \n",
    "    def _backward(self, X, y, H, hprev, Y):\n",
    "        Y[y] -= 1\n",
    "        \n",
    "        self._dWhy += np.dot(H[None].T, Y[None])\n",
    "        self._dby  += Y[None]\n",
    "        \n",
    "        dh = np.dot(Y[None], self._Wy.T) + self._dhnext\n",
    "        dhraw = self._tanh_derivative(H) * dh\n",
    "        assert dhraw.shape == (1, self.h_dim)\n",
    "        \n",
    "        self._dWxh += np.dot(    X[None].T, dhraw)\n",
    "        self._dWhh += np.dot(hprev[None].T, dhraw)\n",
    "        self._dbh  += dhraw\n",
    "        \n",
    "        self._dhnext = np.dot(dhraw, self._Wh.T)\n",
    "        assert self._dhnext.shape == (1, self.h_dim)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        y = np.exp(x - np.max(x))\n",
    "        return y/np.sum(y)\n",
    "    \n",
    "    def _tanh_derivative(self, value):\n",
    "        return 1 - value**2\n",
    "    \n",
    "    def _clip_gradients(self):\n",
    "        for dparam in [self._dWxh, self._dWhh, self._dWhy, self._dbh, self._dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    def forward_backward(self, inputs, targets, hprev):\n",
    "        X = np.zeros((self.seq_len, self.vocab_dim))\n",
    "        for t, input in enumerate(inputs):\n",
    "            X[t, input] = 1\n",
    "        \n",
    "        H = np.zeros((self.seq_len, self.h_dim))\n",
    "        Y = np.zeros_like(X)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for t in range(self.seq_len):\n",
    "            h = hprev if t==0 else H[t-1]\n",
    "            \n",
    "            H[t], Y[t], loss = self._forward(X[t], targets[t], h)\n",
    "            total_loss += loss\n",
    "        \n",
    "        self._dWxh, self._dWhh, self._dWhy = np.zeros_like(self._Wx), np.zeros_like(self._Wh), np.zeros_like(self._Wy)\n",
    "        self._dbh,  self._dby = np.zeros_like(self._bh), np.zeros_like(self._by)\n",
    "        self._dhnext = np.zeros_like(H[0])\n",
    "        \n",
    "        for t in range(self.seq_len)[::-1]:\n",
    "            self._backward(X[t], targets[t], H[t], H[t-1], Y[t])\n",
    "        \n",
    "        self._clip_gradients()\n",
    "        \n",
    "        params  = [self._Wx,   self._Wh,   self._Wy,   self._bh,  self._by]\n",
    "        mparams = [self._mWx,  self._mWh,  self._mWy,  self._mbh, self._mby]\n",
    "        dparams = [self._dWxh, self._dWhh, self._dWhy, self._dbh, self._dby]\n",
    "        \n",
    "#         print(f'Wx: {np.sum(self._Wx):4.4} - Wh: {np.sum(self._Wh):4.4f} - Wy: {np.sum(self._Wy):4.4f}')\n",
    "#         print(f'Wx: {np.sum(self._dWxh):4.4} - Wh: {np.sum(self._dWhh):4.4f} - Wy: {np.sum(self._dWhy):4.4f}')\n",
    "        for param, dparam, mparam in zip(params, dparams, mparams):\n",
    "            mparam += dparam ** 2\n",
    "            param -= self.learning_rate * dparam/np.sqrt(mparam + 1e-8)\n",
    "#         print(f'Wx: {np.sum(self._Wx):4.4} - Wh: {np.sum(self._Wh):4.4f} - Wy: {np.sum(self._Wy):4.4f}')\n",
    "        \n",
    "        return total_loss, H[-1]\n",
    "    \n",
    "    def sample(self, input, hprev, seq_len):\n",
    "        X = np.zeros((1, self.vocab_dim))\n",
    "        X[0, input] = 1\n",
    "        \n",
    "        ixes = []\n",
    "        for t in range(seq_len):\n",
    "            h = np.dot(X, self._Wx) + np.dot(hprev, self._Wh) + self._bh\n",
    "            h = np.tanh(h)\n",
    "            \n",
    "            y = np.dot(h, self._Wy) + self._by\n",
    "            y = self._softmax(y)\n",
    "            \n",
    "            ix = np.random.choice(range(self.vocab_dim), p=y.ravel())\n",
    "            \n",
    "            X *= 0\n",
    "            X[0,ix] = 1\n",
    "            ixes.append(ix)\n",
    "        \n",
    "        print('\\n\\n', ''.join(ix_to_char[ix] for ix in ixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 25\n",
    "threshold = len(data)//25\n",
    "\n",
    "data_supplier = T(data, seq_len)\n",
    "rnn = RNN(vocab_size, 100, seq_len, 1e-1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 484999, loss: 35.608285\n",
      " h! pa o s be flhhí be e da nhhh cho gê eginhelhe de hhí a hus fe a fí chhe qu gu e chhenhhe pre, tufé\n",
      "o ubeme eleu a chhe me po s ca po da bra pigigre be s me go da de fê diça m\n",
      "lhõe be, ra to go etit\n",
      "iter 485621, loss: 48.023473"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "while n <= 1000000:\n",
    "    inputs, targets = data_supplier.get()\n",
    "    if n % threshold == 0:\n",
    "        hprev = np.zeros((1, 100))\n",
    "   \n",
    "    loss, hprev = rnn.forward_backward(inputs, targets, hprev)\n",
    "    \n",
    "    print(f'\\riter {n:2}, loss: {loss:4.6f}', end='')\n",
    "    if (n+1) % 5000 == 0:\n",
    "        rnn.sample(inputs[0], hprev, 200)\n",
    "\n",
    "    n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
